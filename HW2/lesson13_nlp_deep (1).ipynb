{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "lesson13_nlp-deep.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVL4MqVT-8dT"
      },
      "source": [
        "# Lesson 13 - NLP with Deep Learning\n",
        "\n",
        "> An introduction to Deep Learning and its applications in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwIZebi2-8dU"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lewtun/dslectures/blob/master/notebooks/lesson13_nlp-deep.ipynb)[![slides](https://img.shields.io/static/v1?label=slides&message=lesson13_nlp-deep.pdf&color=blue&logo=Google-drive)](https://drive.google.com/open?id=1g613_b3643zUuPVB1JBBuyxXka_WRRWF)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0dY01N0-8dV"
      },
      "source": [
        "> Note: Make sure you are connected to a GPU machine when running the Colab notebook by clicking on `Runtime -> Change runtime type` and set hardware type to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T25McFU-8dV"
      },
      "source": [
        "## Learning objectives\n",
        "In this lecture we cover the basics of Deep Learning and its application to NLP. The learning goals are:\n",
        "* The basics of transfer learning\n",
        "* Preprocess data with the fastai data loader\n",
        "* Train a text classifier with the fastai library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e21sr-n-8dW"
      },
      "source": [
        "## References\n",
        "* Practical Deep Learning for Coders - Lesson 4: Deep Learning 2019 by fastai [[video](https://www.youtube.com/watch?v=qqt3aMPB81c)]\n",
        "\n",
        "This notebooks follows fastai's excellent tutotrial in this video and the original notebook can be found [here](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson3-imdb.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0muezC5-8dW"
      },
      "source": [
        "## Homework\n",
        "As homework read the references, work carefully through the notebook and solve the exercises. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdNiOyh8-8dX"
      },
      "source": [
        "## Introduction\n",
        "Last time we built a sentiment classifier with `scikit-learn` and achieved around 85% accuracy on the test set. This is already pretty good, but can we do better? We are still wrong 15/100 times. It turns out we can if we use deep learning.\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "<img src='https://github.com/lewtun/dslectures/blob/master/notebooks/images/deeper.jpg?raw=1' width='400'>\n",
        "</div>\n",
        "\n",
        "Deep learning uses an architecture that is modeled after the brain and uses networks of artificial neurons to mimic its behaviour. These models are much bigger than the models we encountered so far and can have millions to billions of parameters. Training these models and adjusting the parameters is also more challenging, and generally requires much more data and compute. A lot of computations are easily parallelizable, which is a strength of modern GPUs. Therefore we will run this notebook on a GPU that enables much faster training than a CPU.\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "<img src='https://github.com/lewtun/dslectures/blob/master/notebooks/images/gpu_meme.jpg?raw=1' width='400'>\n",
        "</div>\n",
        "\n",
        "Since we don't have much training data on the IMDb dataset for deep learning standards we use transfer learning to still achieve high accuracy in predicting the sentiment of the movie reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8S4Ebbc-8dX"
      },
      "source": [
        "## Transfer learning\n",
        "Training deep learning models requires a lot of data. It is not uncommon to train models on millions of images or gigabytes of text data to achieve good results. Most real-world problems don't have that amount of labeled data ready, and not all companies or individuals who want to train a model can afford to hire people to label data for them.\n",
        "\n",
        "For many years this has been very challenging. Fortunately, it has been solved for image based models a couple of years ago and recently also for NLP. One approach that helps train models with limited labeled data is called **transfer learning**.\n",
        "\n",
        "The idea is that once a model is trained on a large dataset for a specific task (e.g., classifying houses vs. planes), the model has learned certain features of the data that can be reused for another task. Such features could be how to detect edges or textures in images. If these features are useful for another task, then we can train the model on new data without requiring as many labels as if we were training it from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkYD1CRq-8dX"
      },
      "source": [
        "### ULMFiT\n",
        "For transfer learning in NLP Jeremy Howard and Sebastian Ruder came up with a similar approach called `ULMFiT` (Universal Language Model Fine-tuning for Text Classification) for texts. The central theme of the approach is language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6wC65KG-8dY"
      },
      "source": [
        "#### Language modeling\n",
        "In language modeling the goal is to predict the next word based on the previous word in a text. An example:\n",
        "\n",
        "`Yesterday I discovered a really nice cinema so I went there to watch a ____ .`\n",
        "\n",
        "The task of the model is to fill the blank. The advantage of language modeling is that it does not require any labels, but to achieve good results, the model needs to learn a lot about language. In this example, the model needs to understand that one watches movies in cinemas. The same goes for sentiment and other topics. With `ULMFiT` one can train a language model and then use it for classifications tasks in three steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDFQHVDu-8dY"
      },
      "source": [
        "#### Three steps\n",
        "The three steps are visualised in the following figure:\n",
        "<div style=\"text-align: center\">\n",
        "<img src='https://github.com/lewtun/dslectures/blob/master/notebooks/images/ulmfit_approach.png?raw=1' width='800'>\n",
        "</div>\n",
        "\n",
        "1. **Language model (wiki)**: A language model is trained on a large dataset. Wikipedia is a common choice for this task as it includes many topics, and the text is of high quality. This step usually takes the most time on the order of days. In this step, the model learns the general structure of language.\n",
        "\n",
        "2. **Language model (domain)**: The language model trained on Wikipedia might be missing some aspects of the domain we are interested in. If we want to do sentiment classification, Wikipedia does not offer much insight since Wikipedia articles are generally of neutral sentiment. Therefore, we continue training the language model on the text we are interested in. This step still takes several hours.\n",
        "\n",
        "3. **Classifier (domain)**: Now that the language model works well on the text we are interested in, it is time to build a classifier. We do this by adapting the output of the network to yield classes instead of words. This step only takes a couple of minutes to an hour to complete.\n",
        "\n",
        "The power of this approach is that you only need little labeled data for the last step and only need to go through the expensive first step once. Even the second step can be reused on the dataset if you, for example, build a sentiment classifier and additionally a topic classifier. This can be done in minutes and allows us to achieve great results with little time and resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNIZ_hqX-8dZ"
      },
      "source": [
        "### Other methods\n",
        "Today, there are many approaches in NLP that use transfer learning such as Google's BERT. These models cost on the order of $100'000 to pretrain (1. step) and massive computational facilities. Fortunately, most of these models are shared and can then be fine-tuned on small machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKflcZxl-8dZ"
      },
      "source": [
        "## The `fastai` library\n",
        "The `fastai` library wraps around the deep learning framework `PyTorch` and has a lot of functionality built in to achieve great results quickly. The library abstracts a lot of functionality, so it can be difficult to follow initially. To get a better understanding, we highly recommend the [fastai course](https://course.fast.ai/). In this lesson, we will use the library to build a world-class classifier with just a few lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEfBTEOX-8dZ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7FO1F5z-8da",
        "outputId": "a555c16d-97d3-42b3-fcd8-68fdf72e6a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install dslectures"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dslectures\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/63/65334bbd906733f61daf7b2f6c538d48dd2e264af67343307fb7f7318804/dslectures-0.0.15-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dslectures) (1.18.5)\n",
            "Collecting giotto-tda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/c8/6dac3bc9cc9e3b0b1e735670a614c93fad864ce317a454e38808cc51c993/giotto_tda-0.3.0-cp36-cp36m-manylinux2010_x86_64.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 9.2MB/s \n",
            "\u001b[?25hCollecting black\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/7b/5a6bbe89de849f28d7c109f5ea87b65afa5124ad615f3419e71beb29dc96/black-20.8b1.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 29.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from dslectures) (0.10.1)\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from dslectures) (4.41.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from dslectures) (0.11.0)\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Collecting nbdev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/c5/a02d58ef8c2a75baf3bb873aecff8690d4b9d12c2b23e786dce09ef93f86/nbdev-1.1.5-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from dslectures) (1.1.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from dslectures) (4.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from dslectures) (0.22.2.post1)\n",
            "Requirement already satisfied: sklearn-pandas in /usr/local/lib/python3.6/dist-packages (from dslectures) (1.8.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from dslectures) (3.2.5)\n",
            "Collecting python-igraph>=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/6e/3ac2fc339051f652d4a01570d133e4d15321aaec929ffb5f49a67852f8d9/python_igraph-0.8.3-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.9MB/s \n",
            "\u001b[?25hCollecting plotly>=4.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/af86e9d9bf1a3e4f2dabebeabd02a32e8ddf671a5d072b3af2b011efea99/plotly-4.12.0-py2.py3-none-any.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 315kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from giotto-tda->dslectures) (0.17.0)\n",
            "Collecting scipy>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 118kB/s \n",
            "\u001b[?25hCollecting pyflagser>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/c8/0d3e3713e763b29d45702326cc2a7c0432ce0162194fcf3da29bec82228f/pyflagser-0.4.1-cp36-cp36m-manylinux2010_x86_64.whl (389kB)\n",
            "\u001b[K     |████████████████████████████████| 399kB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.6/dist-packages (from giotto-tda->dslectures) (7.5.1)\n",
            "Collecting regex>=2020.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/39/36e897dd00c3b102326cb4c09f51e1aace2f113dce04dbe3706f19466379/regex-2020.10.28-cp36-cp36m-manylinux2014_x86_64.whl (722kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from black->dslectures) (0.7)\n",
            "Collecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from black->dslectures) (3.7.4.3)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.6/dist-packages (from black->dslectures) (7.1.2)\n",
            "Collecting pathspec<1,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/29/29/a465741a3d97ea3c17d21eaad4c64205428bde56742360876c4391f930d4/pathspec-0.8.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from black->dslectures) (0.10.2)\n",
            "Collecting typed-ast>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 43.7MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->dslectures) (1.24.3)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn->dslectures) (3.2.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (19.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (4.10.1)\n",
            "Requirement already satisfied: nbconvert<6 in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (5.6.1)\n",
            "Collecting fastcore>=1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/66/637874dced27371a0f521c7554bf83658f2ec13a94d64e4db90017973501/fastcore-1.3.2-py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (5.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (20.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (3.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from nbdev->dslectures) (5.3.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->dslectures) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->dslectures) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->dslectures) (1.15.0)\n",
            "Collecting texttable>=1.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.8.2->giotto-tda->dslectures) (1.3.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.1->giotto-tda->dslectures) (4.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.1->giotto-tda->dslectures) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.1->giotto-tda->dslectures) (5.5.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn->dslectures) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn->dslectures) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn->dslectures) (0.10.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->nbdev->dslectures) (5.1.1)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (2.11.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (0.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (4.6.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (2.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev->dslectures) (3.2.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev->dslectures) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev->dslectures) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->nbdev->dslectures) (19.0.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->giotto-tda->dslectures) (4.4.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->giotto-tda->dslectures) (5.3.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (50.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert<6->nbdev->dslectures) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert<6->nbdev->dslectures) (0.5.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->giotto-tda->dslectures) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->giotto-tda->dslectures) (1.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->giotto-tda->dslectures) (0.6.0)\n",
            "Building wheels for collected packages: black\n",
            "  Building wheel for black (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for black: filename=black-20.8b1-cp36-none-any.whl size=124186 sha256=eca1829590fd6ee5239b3e186749010e65cd7dd1803b3a38dcd20790bb293c87\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/10/b5/edf7359c2edd0305cce7e3f96e07daf7ce55dceac9d3ce3373\n",
            "Successfully built black\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=166251346328a404adf4766ff5dec65cccfaef67f3b4c6f9b6874789dfee7c43\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.5.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: giotto-tda 0.3.0 has requirement numpy>=1.19.1, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: giotto-tda 0.3.0 has requirement scikit-learn>=0.23.1, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: texttable, python-igraph, plotly, scipy, pyflagser, giotto-tda, regex, appdirs, pathspec, typed-ast, mypy-extensions, black, selenium, wget, fastcore, nbdev, dslectures\n",
            "  Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed appdirs-1.4.4 black-20.8b1 dslectures-0.0.15 fastcore-1.3.2 giotto-tda-0.3.0 mypy-extensions-0.4.3 nbdev-1.1.5 pathspec-0.8.1 plotly-4.12.0 pyflagser-0.4.1 python-igraph-0.8.3 regex-2020.10.28 scipy-1.5.4 selenium-3.141.0 texttable-1.6.3 typed-ast-1.4.1 wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bePew_YR-8dd"
      },
      "source": [
        "from fastai.text import *\n",
        "from dslectures.core import get_dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hae1RAU5-8df"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP_-ZPGT-8df"
      },
      "source": [
        "### Download data\n",
        "The fastai library includes a lot of datasets including, the IMDb dataset we already know. Similar to our `download_dataset()` function we can do this with fastai's `untar_data()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xLlQHJt-8df",
        "outputId": "11920a1e-b1e3-4188-88fe-028510fdd044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw_LCSYb-8di"
      },
      "source": [
        "### Data structure\n",
        "Looking at the downloaded folder, we can see that there are several files and folders. The relevant ones for our case are `test`, `train`, and `unsup`. The `train` and `test` folders split the data the same way we split it in the previous lecture. The new `unsup` (for unsupervised) folder contains 50k movie reviews that are not classified. We can't use them for training a classifier, but we can use them to fine-tune the language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FitHH0j-8di",
        "outputId": "a1ddd77f-d3cc-447f-c8eb-12e97803d57a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#7) [Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/README'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/unsup')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjWsIlsQ-8dl"
      },
      "source": [
        "Looking at the training folder, we find two folders for the negative and positive movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDdkoyEm-8dm",
        "outputId": "3a7e4e43-3fbf-4f41-e866-a74204f80be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(path/'train/').ls()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#4) [Path('/root/.fastai/data/imdb/train/neg'),Path('/root/.fastai/data/imdb/train/labeledBow.feat'),Path('/root/.fastai/data/imdb/train/unsupBow.feat'),Path('/root/.fastai/data/imdb/train/pos')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA8fdOQY-8do"
      },
      "source": [
        "In both folders, we find many files, each containing one movie review. This is exactly the same data we used last time. It is just arranged in a different structure. We don't need to load all these files manually - the fastai library does this automatically. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMBcs1Z9-8do",
        "outputId": "d77d67e8-153b-4810-9ca5-7b183d9e0109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(path/'train/neg').ls()[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#10) [Path('/root/.fastai/data/imdb/train/neg/6553_4.txt'),Path('/root/.fastai/data/imdb/train/neg/6577_4.txt'),Path('/root/.fastai/data/imdb/train/neg/7009_4.txt'),Path('/root/.fastai/data/imdb/train/neg/6706_1.txt'),Path('/root/.fastai/data/imdb/train/neg/4758_4.txt'),Path('/root/.fastai/data/imdb/train/neg/2025_1.txt'),Path('/root/.fastai/data/imdb/train/neg/7627_4.txt'),Path('/root/.fastai/data/imdb/train/neg/11850_3.txt'),Path('/root/.fastai/data/imdb/train/neg/12493_1.txt'),Path('/root/.fastai/data/imdb/train/neg/1835_2.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McBFMSCl-8dq"
      },
      "source": [
        "## Fine-tune language model\n",
        "\n",
        "> Note: Fine-tuning the language model takes around 4h. You can skip this step and download the fine-tuned model in the *Load language model* section of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7_V2x_G-8dq"
      },
      "source": [
        "### Preprocess data for language modeling\n",
        "In the last lecture, we implemented our own function to preprocess the texts and tokenize them. In principle, we could do the same here, but fastai comes with built-in functions to take care of this. In addtion, we can specify which folders to use and what percentage to split off for validation. The batch size `bs` specifies how many samples the model is optimised for at each step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX9DIArL-8dr"
      },
      "source": [
        "bs=48"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO_6QMk_-8ds",
        "outputId": "60252bc8-502f-4e1a-f3df-99a0022dbe9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "data_lm = (TextList.from_folder(path)\n",
        "           #Inputs: all the text files in path\n",
        "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
        "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
        "            .split_by_rand_pct(0.1)\n",
        "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
        "            .label_for_lm()           \n",
        "           #We want to do a language model so we label accordingly\n",
        "            .databunch(bs=bs))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ircibDnE-8du"
      },
      "source": [
        "Similar to the vectorizer vocabulary, we can have a look at the encoding scheme. The `itos` (stands for id-to-string) object tells us which token is encoded at which position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTXlgGtr-8du",
        "outputId": "3d9a2996-a927-4b64-97d2-5c98487afb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(data_lm.vocab.itos)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFET28X4-8dx"
      },
      "source": [
        "We can see that the vocabulary contains XXX tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgsE7rN6-8dx",
        "outputId": "0507c734-622c-43be-ff94-0050b7e546bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_lm.vocab.itos[:20]\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk',\n",
              " 'xxpad',\n",
              " 'xxbos',\n",
              " 'xxeos',\n",
              " 'xxfld',\n",
              " 'xxmaj',\n",
              " 'xxup',\n",
              " 'xxrep',\n",
              " 'xxwrep',\n",
              " 'the',\n",
              " '.',\n",
              " ',',\n",
              " 'and',\n",
              " 'a',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'it',\n",
              " 'in',\n",
              " 'i']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G05uWp3z-8dz"
      },
      "source": [
        "We can see that the first few positions are reserved for special tokens starting with `xx`. The token `xxunk` is used for a word that is not in the dictionary. The `xxbos` and `xxeos` identify the beginning and the end of a string. So if the first entry in the encoding vector is 1 this means that the token is `xxunk`. If the third entry is 1 then the token is `xxbos`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HjzJoqx-8d0"
      },
      "source": [
        "We can also look at a processed text. We notice that the token `xxmaj` is used frequently. It signifies that the first letter of the following word is capitalised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yHiJDn1-8d0",
        "outputId": "0d38325e-112d-49e8-d356-0df002b5c620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_lm.train_ds[0][0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text [  2  19 698  21 ... 121  76 548  10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u37mRIv3-8d2"
      },
      "source": [
        "With fastai we can also sample a batch from the dataset and display the sample in a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDsALgw5-8d2",
        "outputId": "aa772167-f409-4751-bbd5-52650e0665f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  idx_min = (t != self.pad_idx).nonzero().min()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>the film attempts to tell the story of a dark future , one in which xxmaj hawk ( a xxmaj mad xxmaj max type of character ) heads off to rescue a damsel in distress . xxmaj in reality , the plot is a thinly disguised excuse for the producers to promote their own philosophies on life ( watch the end credits and the ' these people are not real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>guess it was worth my time sitting through this * once * but i wo n't be watching it again . xxmaj there are several things about this film that irritated me . \\n \\n  xxmaj first , man ... i really hated the characters . i had the same problem with xxmaj sid and xxmaj nancy . i have a hard time rationalizing spending a fair chunk of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>xxmaj this film is a cartoonish piece of snot with bright colors and bad mediocre acting . xxmaj was xxmaj mike xxmaj myers even in this movie actually ? xxmaj and another thing , the fish . xxmaj what is with that stupid fish ! xxmaj first time you see him , he 's an actual fish . xxmaj next time you see him , he 's all animated and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>her brother is a scientist who 's trying to deal with a speeding comment headed for xxmaj earth . xxmaj but in the runaway comet business , even a near miss causes some real problems as the xxmaj earth 's orbit goes out of kilter . \\n \\n  xxmaj from the survival of the xxmaj earth we go to the survival of xxmaj van xxmaj dien and his immediate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>xxup tv did n't have such a dominant presence in the industry , this movie would have seemed entertaining . xxmaj but xxmaj mike and xxmaj mark are so obviously playing themselves , xxmaj mike and xxmaj mark . xxmaj at times they are funny and some of the lines seem off the cuff , but mostly they do not ring true . xxmaj they are the reality version of</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THUcPxaa-8d6"
      },
      "source": [
        "### Representation\n",
        "When we vectorized the texts in the last lecture, we represented them as count vectors. The architecture we use in this lecture allows for each word to be processed sequentially and thus conserving the order information. Therefore we encode the text with **one-hot encodings**. However, storing the information as vectors would not be very memory efficient; one entry is 1 and all the other entries are 0. It is more efficient to just store the information on which entry is 1 and then create the vector when we need it.\n",
        "\n",
        "We can look at the data representation of the example we printed above. Each number represents a word in the vocabulary and specifies the entry in the one-hot encoding that is set to one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GhDU_Tx-8d6",
        "outputId": "5d60df05-7bd8-4135-d9f9-f7fb2d11ee25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_lm.train_ds[0][0].data[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,  19, 698,  21,  19,  26,  38,  60,  15,  85])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdRy7ZVB-8d8"
      },
      "source": [
        "With the `itos` we can translate it back to tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx3tuUKC-8d9",
        "outputId": "1179da0c-b578-4f69-a9b1-d0d6fd8c8d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in data_lm.train_ds[0][0].data[:10]:\n",
        "    print(data_lm.vocab.itos[i])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xxbos\n",
            "i\n",
            "knew\n",
            "that\n",
            "i\n",
            "was\n",
            "not\n",
            "about\n",
            "to\n",
            "see\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BSOUkIz-8d_"
      },
      "source": [
        "### Train model\n",
        "We will train a variant of a model called LSTM (long short-term memory) network. This is a neural network with a feedback loop. That means that when fed a sequence of tokens, it feeds back its output for the next prediction. With this the model has a mechanism remembering the past inputs. This is especially useful when dealing with sequential data such as texts, where the sequence of words and characters carries important meaning.\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "<img src='https://github.com/lewtun/dslectures/blob/master/notebooks/images/rnn-diagram.png?raw=1' width='400'>\n",
        "    <p style=\"text-align: center;\"> <b>Reference:</b> https://colah.github.io/posts/2015-08-Understanding-LSTMs/ </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooxr8fZ0-8d_"
      },
      "source": [
        "#### Load pretrained model\n",
        "Training the model on Wikipedia takes a day or two. Fortunately, people have already trained the model and shared it in the fastai library. Therefore, we can just load the pretrained langauge model. When we load it, we also pass the dataset it will be trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lmmw9op-8d_",
        "outputId": "5a2ed3b9-815c-4fa3-8d57-d666be136acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir=\"../data/\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swRWbiyP-8eB"
      },
      "source": [
        "#### Learning rate finder\n",
        "The learning rate is a key parameter when training models in deep learning. It specifies how strongly we update the model parameters. If the learning rate is too small, the training takes forever. If the learning rate is too big, we will never converge to a minimum.\n",
        "\n",
        "With the `lr_find()` function, we can explore how the loss function behaves with regards to the value of the learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbwMZXtc-8eB",
        "outputId": "89f36ea7-1613-4447-cb50-9eb55e86be56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='25' class='' max='8049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.31% [25/8049 06:04<32:27:16 4.7107]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj7c1DGX-8eD"
      },
      "source": [
        "learn.recorder.plot(skip_end=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU_K4Zi3-8eF"
      },
      "source": [
        "First of all, we see that if we choose the learning rate too big, the loss function starts to increase. We want to avoid this at all costs. So we want to find the spot where the loss function decreases the steepest with the largest learning rate. In this case, a good value is `1e-2`. The first parameter determines how many epochs we train. One epoch corresponds to one pass through the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGXPr5W8-8eF"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjLw4HHk-8eH"
      },
      "source": [
        "Deep learning models learn more and more abstractions with each layer. The first layers of an image model might learn about edges and textures in an image, and as you progress through the layers, you can see how the model combines edges to eyes or ears and eventually combines these to faces. Therefore the last few layers are usually the ones that are very task-specific while the others contain general information.\n",
        "\n",
        "For this reason, we usually start by just tuning the last few layers because we don't want to lose that information and then, in the end, fine-tune the whole model. This is what we did above: we just trained the last few layers. Now to get the best possible performance, we want to train the whole model. The `unfreeze()` function enables the training of the whole model. We train the model for 10 more epochs with a slightly lower learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag8YPVxD-8eI"
      },
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cPwKK2m-8eK"
      },
      "source": [
        "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csXTFVf4-8eM"
      },
      "source": [
        "### Save language model\n",
        "Since the step above took about 4h we want to save the progress, so we don't have to repeat the step when we restart the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJUhhwjC-8eM"
      },
      "source": [
        "# uncomment if you want to fine-tune the language mode\n",
        "# data_lm.path = Path('')\n",
        "# data_lm.save('data_lm.pkl')\n",
        "# learn.save('fine_tuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhCsoSBm-8eO"
      },
      "source": [
        "### Load language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Qx0Ur3-8eO"
      },
      "source": [
        "get_dataset(\"fine_tuned.pth\")\n",
        "get_dataset(\"data_lm.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo_t9FhD-8eQ"
      },
      "source": [
        "data_lm = load_data(Path(\"../data/\"), 'data_lm.pkl', bs=bs)\n",
        "data_lm.path = path\n",
        "\n",
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir=\"../data/\")\n",
        "learn.path = Path(\"\")\n",
        "learn.load('fine_tuned');\n",
        "learn.path = path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ3G5N5h-8eS"
      },
      "source": [
        "### Text generation\n",
        "The objective of a language model is to predict the next word based on a sequence of words. We can use the trained model to generate some movie reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUNxwzJR-8eT"
      },
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR5IYauU-8eV"
      },
      "source": [
        "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM4cZdsy-8eW"
      },
      "source": [
        "### Exercise 1\n",
        "Generate a few movie reviews with different input texts. Post the funniest review on the Teams channel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIYM30So-8eX"
      },
      "source": [
        "### Encoder\n",
        "As mentioned previously, the last layers of a deep learning model are usually the most task-specific. In the case of language modeling, the last layer predicts the next word in a sequence. We want to do text classification, however, so we don't need that layer. Therefore, we discard the last layer and only save what is called the encoder. In the next step, we add a new layer on top of the encoder for text classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIC6GyYn-8eX"
      },
      "source": [
        "learn.save_encoder('fine_tuned_enc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43noIRDG-8eZ"
      },
      "source": [
        "## Train classifier\n",
        "In this section, we will use the fine-tuned language model and build a text classifier on top of it.  The procedure is very similar to the language model fine-tuning but needs some minor adjustments for text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7uzpyIm-8eZ"
      },
      "source": [
        "### Preprocess data for classification\n",
        "Preprocessing the data follows similar steps as for language modeling. The main differences are that 1) we don't want a random train/valid split, but the official one and 2) we want to label each element with its sentiment based on the folder name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwg8kWYG-8eZ"
      },
      "source": [
        "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
        "             #grab all the text files in path\n",
        "             .split_by_folder(valid='test')\n",
        "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
        "             .label_from_folder(classes=['neg', 'pos'])\n",
        "             #label them all with their folders\n",
        "             .databunch(bs=bs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbLE2AHm-8eb"
      },
      "source": [
        "When we display a batch we see that the tokens look the same with the addition of a label column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELby3QBM-8eb"
      },
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd7iX4oc-8ee"
      },
      "source": [
        "### Load model\n",
        "We create a text classifier model and load the pretrained encoder part from the fine-tuned language model into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDFF3Km--8ee"
      },
      "source": [
        "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir=\"../data/\")\n",
        "learn.load_encoder('fine_tuned_enc');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH0NavCv-8eg"
      },
      "source": [
        "### Find the learning rate\n",
        "Again, we need to find the best learning rate for training the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_rtH8YS-8eh"
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrtgOCd8-8ei"
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SBemrat-8el"
      },
      "source": [
        "### Train classifier\n",
        "We start by just training the very last layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAAGOZMx-8el"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtqqBhXp-8eo"
      },
      "source": [
        "Then we unfreeze the second to last layer as well and train these layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwBoYwGv-8eo"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_phGlbAd-8eq"
      },
      "source": [
        "Next, we train the last three layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSTZa7_A-8er"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLo9SI6J-8eu"
      },
      "source": [
        "Finally, we train all layers for two epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d7puI-Z-8eu"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551HsdZp-8ew"
      },
      "source": [
        "Note that after the last optimisation step the model can predict the sentiment on the test set with **94%** accuracy. This is roughly 10% better than our Naïve Bayes model from the last lecture. In other words, this model makes **3 times** fewer mistakes than the Naïve Bayes model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwO0_j2i-8ex"
      },
      "source": [
        "### Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0RoMKEj-8ex"
      },
      "source": [
        "learn.predict(\"I really loved that movie, it was awesome!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzSFOA6d-8ez"
      },
      "source": [
        "### Exercise 2\n",
        "Experiment with the trained classifier and see if you can fool it. Can you find a pattern that fools it consistently?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfoKkaW_-8e0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}